{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "plant-seedlings-classification.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "I5AI3AMIvtpA"
      ]
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcIHMDIwvto6"
      },
      "source": [
        "# 1. Setting up dependencies and config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "Xgh5-K58vto8"
      },
      "source": [
        "import os\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt \n",
        "\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "\n",
        "import tensorflow as tf\n",
        "from keras.models import Model\n",
        "from keras.preprocessing import image\n",
        "from keras.applications.xception import *\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.layers import Dropout, Dense, GlobalAveragePooling2D"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "SciunoRdvto_"
      },
      "source": [
        "# Defining configuration variables\n",
        "labels = ['Black-grass',\n",
        "          'Charlock',\n",
        "          'Cleavers',\n",
        "          'Common Chickweed',\n",
        "          'Common wheat',\n",
        "          'Fat Hen',\n",
        "          'Loose Silky-bent',\n",
        "          'Maize',\n",
        "          'Scentless Mayweed',\n",
        "          'Shepherds Purse',\n",
        "          'Small-flowered Cranesbill',\n",
        "          'Sugar beet']\n",
        "\n",
        "# Enable/Disable data manipulation\n",
        "GEN_DATA = False\n",
        "UNZIP = False\n",
        "\n",
        "# Version of the model being trained\n",
        "DATA_VERSION = \"\"\n",
        "SAVE_VERSION = \"4\"\n",
        "\n",
        "# Defining data paths\n",
        "DATA_DIR = 'drive/My Drive/plant-seedlings-classification'\n",
        "TRAIN_SEG_PATH = f'{DATA_DIR}/train_seg{DATA_VERSION}'\n",
        "TEST_SEG_PATH = f'{DATA_DIR}/test_seg{DATA_VERSION}'\n",
        "\n",
        "# Path to store models and submissions\n",
        "MODEL_PATH = f'{DATA_DIR}/Xception{SAVE_VERSION}.h5'\n",
        "SUBMISSION_PATH = f'{DATA_DIR}/submission{SAVE_VERSION}.csv'\n",
        "\n",
        "# Defining model hyperparameters\n",
        "IMG_SIZE = 299\n",
        "BATCH_SIZE = 16\n",
        "TRAIN_IMG_COUNT = 4750\n",
        "EPOCHS = 300\n",
        "MODEL_COUNT=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49PNbfElJMY0",
        "outputId": "4b15ae9f-5937-44c2-ba80-8b42ece82c37"
      },
      "source": [
        "# Mounting Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive/\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTXKt3cZLgOV"
      },
      "source": [
        "# Unzipping the data if not already done\n",
        "if UNZIP:\n",
        "  ! unzip -uq \"drive/MyDrive/plant-seedlings-classification/test.zip\" -d \"drive/MyDrive/plant-seedlings-classification/test\"\n",
        "  ! unzip -uq \"drive/MyDrive/plant-seedlings-classification/train.zip\" -d \"drive/MyDrive/plant-seedlings-classification/train\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5AI3AMIvtpA"
      },
      "source": [
        "# 2. Segmenting the images\n",
        "\n",
        "We iterate through all train and test set images and store their segmented versions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-g4SglomTZLv"
      },
      "source": [
        "# Defining a function to equalize the image's histogram\n",
        "def equalize(img):\n",
        "  img_YCrCb = cv2.cvtColor(img,cv2.COLOR_BGR2YCR_CB)\n",
        "  channels = cv2.split(img_YCrCb)\n",
        "  cv2.equalizeHist(channels[0],channels[0])\n",
        "  cv2.merge(channels, img_YCrCb)\n",
        "  cv2.cvtColor(img_YCrCb, cv2.COLOR_YCR_CB2BGR, img)\n",
        "  return img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "UB3Kfi7-vtpB"
      },
      "source": [
        "# Defining a function to create a mask \n",
        "def create_mask(img):\n",
        "    img_hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
        "    \n",
        "    sensitivity = 35\n",
        "    hsv_low = np.array([60 - sensitivity, 100, 50])\n",
        "    hsv_high = np.array([60 + sensitivity, 255, 255])\n",
        "    \n",
        "    mask = cv2.inRange(img_hsv, hsv_low, hsv_high)\n",
        "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (11,11))\n",
        "    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n",
        "    \n",
        "    return mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "FfM0zYvvvtpC"
      },
      "source": [
        "# Defining a function to segment an image\n",
        "def segment(img):\n",
        "    mask = create_mask(img)\n",
        "    masked_img = cv2.bitwise_and(img, img, mask = mask)\n",
        "    return masked_img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "EIXdc_yTvtpC"
      },
      "source": [
        "# Defining a function to sharpen an image\n",
        "def sharpen(img):\n",
        "    img_blur = cv2.GaussianBlur(img, (0,0), 3)\n",
        "    img_sharp = cv2.addWeighted(img, 1.5, img_blur, -0.5, 0)\n",
        "    return img_sharp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "_seOB4c_vtpE"
      },
      "source": [
        "if GEN_DATA:\n",
        "  # Creating the folder to save segmented training images\n",
        "  if not os.path.isdir(TRAIN_SEG_PATH):\n",
        "      os.mkdir(TRAIN_SEG_PATH)\n",
        "      \n",
        "  f, axarr = plt.subplots(1,2) # For showing a sample image\n",
        "\n",
        "  # Segmenting the training data       \n",
        "  for idx, label in enumerate(labels):\n",
        "      \n",
        "      folder = os.path.join(DATA_DIR, \"train\", label)\n",
        "      \n",
        "      show_img = True\n",
        "      for img_name in os.listdir(folder):\n",
        "          img_path = os.path.join(folder, img_name)\n",
        "          img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
        "          img_eq = equalize(img)\n",
        "          img_seg = segment(img_eq)\n",
        "          img_sharp = sharpen(img_seg)\n",
        "          seg_path = os.path.join(TRAIN_SEG_PATH, label)\n",
        "          if not os.path.isdir(seg_path):\n",
        "              os.mkdir(seg_path)\n",
        "          cv2.imwrite(os.path.join(seg_path, img_name), img_sharp)\n",
        "          if show_img: # Plotting a sample\n",
        "              show_img = False\n",
        "              axarr[0].imshow(img)\n",
        "              axarr[1].imshow(img_seg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "NpPuwLwWvtpF"
      },
      "source": [
        "if GEN_DATA:\n",
        "  # Creating the folder to save segmented testing images\n",
        "  if not os.path.isdir(TEST_SEG_PATH):\n",
        "      os.mkdir(TEST_SEG_PATH)\n",
        "  \n",
        "  # Segmenting the testing data  \n",
        "  folder = os.path.join(DATA_DIR, \"test\")\n",
        "  for img_name in os.listdir(folder):\n",
        "      img_path = os.path.join(folder, img_name)\n",
        "      img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
        "      img_eq = equalize(img)\n",
        "      img_seg = segment(img_eq)\n",
        "      img_sharp = sharpen(img_seg)\n",
        "      seg_path = TEST_SEG_PATH\n",
        "      cv2.imwrite(os.path.join(seg_path, img_name), img_sharp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKBY_QXCvtpG"
      },
      "source": [
        "# 3. Training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "7qgSJQmCvtph"
      },
      "source": [
        "# Creating a data generator with data augmentation rules\n",
        "# datagen = ImageDataGenerator(preprocessing_function=preprocess_input,\n",
        "#                              height_shift_range=0.3,\n",
        "#                              horizontal_flip=True,\n",
        "#                              rotation_range=180,\n",
        "#                              vertical_flip=True,\n",
        "#                              width_shift_range=0.3,\n",
        "#                              zoom_range=0.3)\n",
        "\n",
        "datagen = ImageDataGenerator(preprocessing_function=preprocess_input,\n",
        "                             brightness_range=[0.5,1.5],\n",
        "                             height_shift_range=0.3,\n",
        "                             horizontal_flip=True,\n",
        "                             rotation_range=180,\n",
        "                             shear_range=0.2,\n",
        "                             vertical_flip=True,\n",
        "                             width_shift_range=0.3,\n",
        "                             zoom_range=0.3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "ROD0JoSxvtph"
      },
      "source": [
        "# For defining and compiling the models\n",
        "def create_model():\n",
        "  initial_model = Xception(weights='imagenet', input_shape=(IMG_SIZE, IMG_SIZE, 3), include_top=False)\n",
        "  x = initial_model.output\n",
        "  x = GlobalAveragePooling2D()(x)\n",
        "  x = Dropout(0.5)(x)\n",
        "  x = Dense(1024, activation='relu')(x)\n",
        "  x = Dropout(0.5)(x)\n",
        "  outputs = Dense(12, activation='softmax')(x)\n",
        "  model = Model(inputs=initial_model.input, outputs=outputs)\n",
        "\n",
        "  model.compile(optimizer='Adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "  \n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTkzKEigPVyM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db769b92-8342-4cf7-989b-af9157637802"
      },
      "source": [
        "# Creating the list of models\n",
        "models = []\n",
        "\n",
        "for i in range(MODEL_COUNT):\n",
        "  models.append(create_model())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/xception/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "83689472/83683744 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PV5ajnLGPZ-Z",
        "outputId": "52b4e314-1cc2-43cd-c52f-8b05b9c6ba42"
      },
      "source": [
        "# Initializing the data generator\n",
        "train_gen = datagen.flow_from_directory(TRAIN_SEG_PATH, \n",
        "                                       target_size=(IMG_SIZE, IMG_SIZE), \n",
        "                                       batch_size=BATCH_SIZE,\n",
        "                                       class_mode='categorical')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 4750 images belonging to 12 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHx3fvEf6Q7Z"
      },
      "source": [
        "# Defining the learning rate scheduler\n",
        "def scheduler(epoch, lr):\n",
        "  print(lr)\n",
        "  if epoch < 6:\n",
        "    return lr\n",
        "  else:\n",
        "    # return lr * tf.math.exp(-0.1)\n",
        "    return lr*0.9"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bDHr-denvtpi",
        "outputId": "eaadb74a-43d7-4ce4-e523-3040c568f71d"
      },
      "source": [
        "# Training the models\n",
        "for i in range(MODEL_COUNT):\n",
        "  curr_model_path = f'{DATA_DIR}/Xception{SAVE_VERSION}_{i}.h5'\n",
        "\n",
        "  lr_callback = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
        "\n",
        "  # Defining the checkpoint saving callback\n",
        "  ckpt_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "      filepath=curr_model_path,\n",
        "      save_weights_only=True,\n",
        "      monitor='accuracy',\n",
        "      mode='max',\n",
        "      save_freq=TRAIN_IMG_COUNT//(10*BATCH_SIZE),\n",
        "      save_best_only=True)\n",
        "\n",
        "  print(f\"=== Model number: {i+1} ===\")\n",
        "  models[i].fit_generator(train_gen, steps_per_epoch=TRAIN_IMG_COUNT//BATCH_SIZE, epochs=EPOCHS, callbacks=[lr_callback, ckpt_callback], verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "=== Model number: 1 ===\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/300\n",
            "0.0010000000474974513\n",
            "296/296 [==============================] - 1743s 6s/step - loss: 1.9901 - accuracy: 0.3359\n",
            "Epoch 2/300\n",
            "0.0010000000474974513\n",
            "296/296 [==============================] - 190s 639ms/step - loss: 0.9573 - accuracy: 0.6811\n",
            "Epoch 3/300\n",
            "0.0010000000474974513\n",
            "296/296 [==============================] - 179s 602ms/step - loss: 0.6821 - accuracy: 0.7751\n",
            "Epoch 4/300\n",
            "0.0010000000474974513\n",
            "296/296 [==============================] - 179s 605ms/step - loss: 0.5498 - accuracy: 0.8112\n",
            "Epoch 5/300\n",
            "0.0010000000474974513\n",
            "296/296 [==============================] - 179s 604ms/step - loss: 0.5651 - accuracy: 0.8263\n",
            "Epoch 6/300\n",
            "0.0010000000474974513\n",
            "296/296 [==============================] - 182s 614ms/step - loss: 0.4774 - accuracy: 0.8403\n",
            "Epoch 7/300\n",
            "0.0010000000474974513\n",
            "296/296 [==============================] - 181s 610ms/step - loss: 0.4170 - accuracy: 0.8695\n",
            "Epoch 8/300\n",
            "0.0009000000427477062\n",
            "296/296 [==============================] - 179s 605ms/step - loss: 0.3570 - accuracy: 0.8789\n",
            "Epoch 9/300\n",
            "0.0008100000559352338\n",
            "296/296 [==============================] - 180s 609ms/step - loss: 0.2981 - accuracy: 0.8995\n",
            "Epoch 10/300\n",
            "0.0007290000794455409\n",
            "296/296 [==============================] - 178s 601ms/step - loss: 0.2716 - accuracy: 0.9042\n",
            "Epoch 11/300\n",
            "0.0006561000482179224\n",
            "296/296 [==============================] - 178s 600ms/step - loss: 0.2433 - accuracy: 0.9201\n",
            "Epoch 12/300\n",
            "0.0005904900608584285\n",
            "296/296 [==============================] - 178s 600ms/step - loss: 0.2506 - accuracy: 0.9110\n",
            "Epoch 13/300\n",
            "0.0005314410664141178\n",
            "296/296 [==============================] - 179s 604ms/step - loss: 0.2082 - accuracy: 0.9292\n",
            "Epoch 14/300\n",
            "0.00047829694813117385\n",
            "296/296 [==============================] - 179s 603ms/step - loss: 0.1876 - accuracy: 0.9310\n",
            "Epoch 15/300\n",
            "0.00043046724749729037\n",
            "296/296 [==============================] - 179s 604ms/step - loss: 0.1689 - accuracy: 0.9444\n",
            "Epoch 16/300\n",
            "0.0003874205285683274\n",
            "296/296 [==============================] - 178s 599ms/step - loss: 0.1748 - accuracy: 0.9399\n",
            "Epoch 17/300\n",
            "0.0003486784698907286\n",
            "296/296 [==============================] - 180s 608ms/step - loss: 0.1618 - accuracy: 0.9442\n",
            "Epoch 18/300\n",
            "0.0003138106258120388\n",
            "296/296 [==============================] - 177s 599ms/step - loss: 0.1488 - accuracy: 0.9429\n",
            "Epoch 19/300\n",
            "0.00028242956614121795\n",
            "296/296 [==============================] - 178s 600ms/step - loss: 0.1553 - accuracy: 0.9436\n",
            "Epoch 20/300\n",
            "0.00025418659788556397\n",
            "296/296 [==============================] - 182s 613ms/step - loss: 0.1542 - accuracy: 0.9505\n",
            "Epoch 21/300\n",
            "0.00022876793809700757\n",
            "296/296 [==============================] - 180s 607ms/step - loss: 0.1113 - accuracy: 0.9606\n",
            "Epoch 22/300\n",
            "0.00020589114865288138\n",
            "296/296 [==============================] - 179s 603ms/step - loss: 0.1285 - accuracy: 0.9524\n",
            "Epoch 23/300\n",
            "0.0001853020366979763\n",
            "296/296 [==============================] - 178s 600ms/step - loss: 0.1305 - accuracy: 0.9495\n",
            "Epoch 24/300\n",
            "0.00016677183157298714\n",
            "296/296 [==============================] - 179s 604ms/step - loss: 0.1047 - accuracy: 0.9614\n",
            "Epoch 25/300\n",
            "0.000150094652781263\n",
            "296/296 [==============================] - 179s 604ms/step - loss: 0.1048 - accuracy: 0.9650\n",
            "Epoch 26/300\n",
            "0.0001350851816823706\n",
            "296/296 [==============================] - 177s 598ms/step - loss: 0.1126 - accuracy: 0.9607\n",
            "Epoch 27/300\n",
            "0.0001215766606037505\n",
            "296/296 [==============================] - 177s 598ms/step - loss: 0.1020 - accuracy: 0.9626\n",
            "Epoch 28/300\n",
            "0.00010941899381577969\n",
            "296/296 [==============================] - 178s 601ms/step - loss: 0.0966 - accuracy: 0.9647\n",
            "Epoch 29/300\n",
            "9.847709588939324e-05\n",
            "296/296 [==============================] - 179s 602ms/step - loss: 0.0980 - accuracy: 0.9683\n",
            "Epoch 30/300\n",
            "8.862938557285815e-05\n",
            " 66/296 [=====>........................] - ETA: 2:18 - loss: 0.0830 - accuracy: 0.9680"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnCHDsS0vtpi"
      },
      "source": [
        "# 4. Testing the model and saving the submission"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "UmGRruZRvtpk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70b98a19-8686-4a2c-8eb6-a22de96361e6"
      },
      "source": [
        "# Loading the model weights\n",
        "models = []\n",
        "for i in range(MODEL_COUNT-1):\n",
        "  model = create_model()\n",
        "\n",
        "  curr_model_path = f'{DATA_DIR}/Xception{SAVE_VERSION}_{i}.h5'\n",
        "  model.load_weights(curr_model_path)\n",
        "  models.append(model)\n",
        "\n",
        "with open(SUBMISSION_PATH, 'w') as f:\n",
        "  f.write('file,species\\n')\n",
        "  for file in tqdm(os.listdir(TEST_SEG_PATH)):\n",
        "    img = image.load_img(os.path.join(TEST_SEG_PATH, file), target_size=(IMG_SIZE, IMG_SIZE))\n",
        "    x = image.img_to_array(img)\n",
        "    x = np.expand_dims(x, axis=0)\n",
        "    pred = np.zeros([12,])\n",
        "    for model in models:\n",
        "      for i, im in enumerate(datagen.flow(x)):\n",
        "          pred += model.predict(im)[0]\n",
        "          if i > 100:\n",
        "              break\n",
        "    f.write('{},{}\\n'.format(file, labels[np.where(pred==np.max(pred))[0][0]]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 794/794 [2:47:02<00:00, 12.62s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}